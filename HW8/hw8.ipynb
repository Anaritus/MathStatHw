{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "755024ce",
   "metadata": {},
   "source": [
    "# Семинар"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d9a5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "from typing import Callable\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a3d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "472fca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerCalc(sigma: float, alpha: float) -> float:\n",
    "    Ca = ss.norm.ppf(1 - alpha, loc = 0, scale = sigma)\n",
    "    return ss.norm.sf(Ca, loc = 1, scale = sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b8c11de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(power: float) -> Callable[[float], float]:\n",
    "    alpha = 0.05\n",
    "    sigma = bisect(lambda x: powerCalc(x, alpha) - power, 1e-5, 20000)\n",
    "    print(sigma)\n",
    "    return lambda x: powerCalc(sigma, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4cb704c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30397841595570746\n"
     ]
    }
   ],
   "source": [
    "Power = foo(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e3491f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9500000000001977"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Power(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c43161c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "y = Power(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0577e151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c69b137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4021757934803809\n"
     ]
    }
   ],
   "source": [
    "from scipy import integrate\n",
    "from scipy.misc import derivative\n",
    "newPower = foo(0.8)\n",
    "result = newPower(0.05) ** 2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dd31df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31999999999976914"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "268c8c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11411645739008325"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - newPower(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afffddbd",
   "metadata": {},
   "source": [
    "# Домашка.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74d3d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "power = 0.95\n",
    "n = bisect(lambda x: powerCalc(1 / np.sqrt(x), 0.05) - power, 5, 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "583b10b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.82217381638289"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8935353d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.18784524,  0.11908195, -0.03649914, ...,  0.51508914,\n",
       "       -0.78721648, -0.23171545])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = ss.norm.rvs(loc = 0, scale = 1, size=(10000, 5))\n",
    "Y = ss.norm.rvs(loc = 0, scale = 1, size=(10000, 1))\n",
    "Z = np.concatenate((X, Y), axis = 1)\n",
    "Z.shape\n",
    "Z.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24265893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b31ab091",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-2513c4d9baf7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcur\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcur\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0minds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "cur = np.array(range(5))\n",
    "inds = np.array([2, 4])\n",
    "cur[not inds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2dd59303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "950187\n",
      "933245\n",
      "920525\n",
      "909891\n",
      "900998\n",
      "893114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.11395100000000014, 0.11395100000000002)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BIG = int(1e6)\n",
    "X = ss.norm.rvs(loc = 0, scale = 1, size=(BIG, 4), random_state=38)\n",
    "cur = X.mean(axis=1)\n",
    "pacc = []\n",
    "Cs = np.array(range(BIG))\n",
    "for n in range(5, 12):\n",
    "    Y = ss.norm.rvs(loc = 0, scale = 1, size=(BIG, 1), random_state=n)\n",
    "    X = np.concatenate((X, Y), axis=1)\n",
    "    cur = X.mean(axis=1)\n",
    "    Ca = ss.norm.ppf(0.95, loc=0, scale = 1 / np.sqrt(n))\n",
    "    print(len(Cs))\n",
    "    denum = len(Cs)\n",
    "    Cs = Cs[np.where(cur[Cs] <= Ca)]\n",
    "    num = len(Cs)\n",
    "    pacc.append(num / denum)\n",
    "prod = 1\n",
    "for t in pacc:\n",
    "    prod *= t\n",
    "    \n",
    "1 - prod, 1 - len(Cs) / BIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7b8b9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b3fdface",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function minimize in module scipy.optimize._minimize:\n",
      "\n",
      "minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
      "    Minimization of scalar function of one or more variables.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    fun : callable\n",
      "        The objective function to be minimized.\n",
      "    \n",
      "            ``fun(x, *args) -> float``\n",
      "    \n",
      "        where ``x`` is an 1-D array with shape (n,) and ``args``\n",
      "        is a tuple of the fixed parameters needed to completely\n",
      "        specify the function.\n",
      "    x0 : ndarray, shape (n,)\n",
      "        Initial guess. Array of real elements of size (n,),\n",
      "        where 'n' is the number of independent variables.\n",
      "    args : tuple, optional\n",
      "        Extra arguments passed to the objective function and its\n",
      "        derivatives (`fun`, `jac` and `hess` functions).\n",
      "    method : str or callable, optional\n",
      "        Type of solver.  Should be one of\n",
      "    \n",
      "            - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
      "            - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
      "            - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
      "            - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
      "            - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
      "            - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
      "            - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
      "            - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
      "            - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
      "            - 'trust-constr':ref:`(see here) <optimize.minimize-trustconstr>`\n",
      "            - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
      "            - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
      "            - 'trust-exact' :ref:`(see here) <optimize.minimize-trustexact>`\n",
      "            - 'trust-krylov' :ref:`(see here) <optimize.minimize-trustkrylov>`\n",
      "            - custom - a callable object (added in version 0.14.0),\n",
      "              see below for description.\n",
      "    \n",
      "        If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
      "        depending if the problem has constraints or bounds.\n",
      "    jac : {callable,  '2-point', '3-point', 'cs', bool}, optional\n",
      "        Method for computing the gradient vector. Only for CG, BFGS,\n",
      "        Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov,\n",
      "        trust-exact and trust-constr.\n",
      "        If it is a callable, it should be a function that returns the gradient\n",
      "        vector:\n",
      "    \n",
      "            ``jac(x, *args) -> array_like, shape (n,)``\n",
      "    \n",
      "        where ``x`` is an array with shape (n,) and ``args`` is a tuple with\n",
      "        the fixed parameters. If `jac` is a Boolean and is True, `fun` is\n",
      "        assumed to return and objective and gradient as an ``(f, g)`` tuple.\n",
      "        Methods 'Newton-CG', 'trust-ncg', 'dogleg', 'trust-exact', and\n",
      "        'trust-krylov' require that either a callable be supplied, or that\n",
      "        `fun` return the objective and gradient.\n",
      "        If None or False, the gradient will be estimated using 2-point finite\n",
      "        difference estimation with an absolute step size.\n",
      "        Alternatively, the keywords  {'2-point', '3-point', 'cs'} can be used\n",
      "        to select a finite difference scheme for numerical estimation of the\n",
      "        gradient with a relative step size. These finite difference schemes\n",
      "        obey any specified `bounds`.\n",
      "    hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy}, optional\n",
      "        Method for computing the Hessian matrix. Only for Newton-CG, dogleg,\n",
      "        trust-ncg,  trust-krylov, trust-exact and trust-constr. If it is\n",
      "        callable, it should return the  Hessian matrix:\n",
      "    \n",
      "            ``hess(x, *args) -> {LinearOperator, spmatrix, array}, (n, n)``\n",
      "    \n",
      "        where x is a (n,) ndarray and `args` is a tuple with the fixed\n",
      "        parameters. LinearOperator and sparse matrix returns are\n",
      "        allowed only for 'trust-constr' method. Alternatively, the keywords\n",
      "        {'2-point', '3-point', 'cs'} select a finite difference scheme\n",
      "        for numerical estimation. Or, objects implementing\n",
      "        `HessianUpdateStrategy` interface can be used to approximate\n",
      "        the Hessian. Available quasi-Newton methods implementing\n",
      "        this interface are:\n",
      "    \n",
      "            - `BFGS`;\n",
      "            - `SR1`.\n",
      "    \n",
      "        Whenever the gradient is estimated via finite-differences,\n",
      "        the Hessian cannot be estimated with options\n",
      "        {'2-point', '3-point', 'cs'} and needs to be\n",
      "        estimated using one of the quasi-Newton strategies.\n",
      "        Finite-difference options {'2-point', '3-point', 'cs'} and\n",
      "        `HessianUpdateStrategy` are available only for 'trust-constr' method.\n",
      "    hessp : callable, optional\n",
      "        Hessian of objective function times an arbitrary vector p. Only for\n",
      "        Newton-CG, trust-ncg, trust-krylov, trust-constr.\n",
      "        Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
      "        provided, then `hessp` will be ignored.  `hessp` must compute the\n",
      "        Hessian times an arbitrary vector:\n",
      "    \n",
      "            ``hessp(x, p, *args) ->  ndarray shape (n,)``\n",
      "    \n",
      "        where x is a (n,) ndarray, p is an arbitrary vector with\n",
      "        dimension (n,) and `args` is a tuple with the fixed\n",
      "        parameters.\n",
      "    bounds : sequence or `Bounds`, optional\n",
      "        Bounds on variables for L-BFGS-B, TNC, SLSQP, Powell, and\n",
      "        trust-constr methods. There are two ways to specify the bounds:\n",
      "    \n",
      "            1. Instance of `Bounds` class.\n",
      "            2. Sequence of ``(min, max)`` pairs for each element in `x`. None\n",
      "               is used to specify no bound.\n",
      "    \n",
      "    constraints : {Constraint, dict} or List of {Constraint, dict}, optional\n",
      "        Constraints definition (only for COBYLA, SLSQP and trust-constr).\n",
      "    \n",
      "        Constraints for 'trust-constr' are defined as a single object or a\n",
      "        list of objects specifying constraints to the optimization problem.\n",
      "        Available constraints are:\n",
      "    \n",
      "            - `LinearConstraint`\n",
      "            - `NonlinearConstraint`\n",
      "    \n",
      "        Constraints for COBYLA, SLSQP are defined as a list of dictionaries.\n",
      "        Each dictionary with fields:\n",
      "    \n",
      "            type : str\n",
      "                Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "            fun : callable\n",
      "                The function defining the constraint.\n",
      "            jac : callable, optional\n",
      "                The Jacobian of `fun` (only for SLSQP).\n",
      "            args : sequence, optional\n",
      "                Extra arguments to be passed to the function and Jacobian.\n",
      "    \n",
      "        Equality constraint means that the constraint function result is to\n",
      "        be zero whereas inequality means that it is to be non-negative.\n",
      "        Note that COBYLA only supports inequality constraints.\n",
      "    tol : float, optional\n",
      "        Tolerance for termination. For detailed control, use solver-specific\n",
      "        options.\n",
      "    options : dict, optional\n",
      "        A dictionary of solver options. All methods accept the following\n",
      "        generic options:\n",
      "    \n",
      "            maxiter : int\n",
      "                Maximum number of iterations to perform. Depending on the\n",
      "                method each iteration may use several function evaluations.\n",
      "            disp : bool\n",
      "                Set to True to print convergence messages.\n",
      "    \n",
      "        For method-specific options, see :func:`show_options()`.\n",
      "    callback : callable, optional\n",
      "        Called after each iteration. For 'trust-constr' it is a callable with\n",
      "        the signature:\n",
      "    \n",
      "            ``callback(xk, OptimizeResult state) -> bool``\n",
      "    \n",
      "        where ``xk`` is the current parameter vector. and ``state``\n",
      "        is an `OptimizeResult` object, with the same fields\n",
      "        as the ones from the return. If callback returns True\n",
      "        the algorithm execution is terminated.\n",
      "        For all the other methods, the signature is:\n",
      "    \n",
      "            ``callback(xk)``\n",
      "    \n",
      "        where ``xk`` is the current parameter vector.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    res : OptimizeResult\n",
      "        The optimization result represented as a ``OptimizeResult`` object.\n",
      "        Important attributes are: ``x`` the solution array, ``success`` a\n",
      "        Boolean flag indicating if the optimizer exited successfully and\n",
      "        ``message`` which describes the cause of the termination. See\n",
      "        `OptimizeResult` for a description of other attributes.\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    minimize_scalar : Interface to minimization algorithms for scalar\n",
      "        univariate functions\n",
      "    show_options : Additional options accepted by the solvers\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This section describes the available solvers that can be selected by the\n",
      "    'method' parameter. The default method is *BFGS*.\n",
      "    \n",
      "    **Unconstrained minimization**\n",
      "    \n",
      "    Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
      "    Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
      "    applications. However, if numerical computation of derivative can be\n",
      "    trusted, other algorithms using the first and/or second derivatives\n",
      "    information might be preferred for their better performance in\n",
      "    general.\n",
      "    \n",
      "    Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
      "    gradient algorithm by Polak and Ribiere, a variant of the\n",
      "    Fletcher-Reeves method described in [5]_ pp.120-122. Only the\n",
      "    first derivatives are used.\n",
      "    \n",
      "    Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
      "    method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
      "    pp. 136. It uses the first derivatives only. BFGS has proven good\n",
      "    performance even for non-smooth optimizations. This method also\n",
      "    returns an approximation of the Hessian inverse, stored as\n",
      "    `hess_inv` in the OptimizeResult object.\n",
      "    \n",
      "    Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
      "    Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
      "    Newton method). It uses a CG method to the compute the search\n",
      "    direction. See also *TNC* method for a box-constrained\n",
      "    minimization with a similar algorithm. Suitable for large-scale\n",
      "    problems.\n",
      "    \n",
      "    Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
      "    trust-region algorithm [5]_ for unconstrained minimization. This\n",
      "    algorithm requires the gradient and Hessian; furthermore the\n",
      "    Hessian is required to be positive definite.\n",
      "    \n",
      "    Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
      "    Newton conjugate gradient trust-region algorithm [5]_ for\n",
      "    unconstrained minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector. Suitable for large-scale problems.\n",
      "    \n",
      "    Method :ref:`trust-krylov <optimize.minimize-trustkrylov>` uses\n",
      "    the Newton GLTR trust-region algorithm [14]_, [15]_ for unconstrained\n",
      "    minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector. Suitable for large-scale problems.\n",
      "    On indefinite problems it requires usually less iterations than the\n",
      "    `trust-ncg` method and is recommended for medium and large-scale problems.\n",
      "    \n",
      "    Method :ref:`trust-exact <optimize.minimize-trustexact>`\n",
      "    is a trust-region method for unconstrained minimization in which\n",
      "    quadratic subproblems are solved almost exactly [13]_. This\n",
      "    algorithm requires the gradient and the Hessian (which is\n",
      "    *not* required to be positive definite). It is, in many\n",
      "    situations, the Newton method to converge in fewer iteraction\n",
      "    and the most recommended for small and medium-size problems.\n",
      "    \n",
      "    **Bound-Constrained minimization**\n",
      "    \n",
      "    Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
      "    algorithm [6]_, [7]_ for bound constrained minimization.\n",
      "    \n",
      "    Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
      "    of Powell's method [3]_, [4]_ which is a conjugate direction\n",
      "    method. It performs sequential one-dimensional minimizations along\n",
      "    each vector of the directions set (`direc` field in `options` and\n",
      "    `info`), which is updated at each iteration of the main\n",
      "    minimization loop. The function need not be differentiable, and no\n",
      "    derivatives are taken. If bounds are not provided, then an\n",
      "    unbounded line search will be used. If bounds are provided and\n",
      "    the initial guess is within the bounds, then every function\n",
      "    evaluation throughout the minimization procedure will be within\n",
      "    the bounds. If bounds are provided, the initial guess is outside\n",
      "    the bounds, and `direc` is full rank (default has full rank), then\n",
      "    some function evaluations during the first iteration may be\n",
      "    outside the bounds, but every function evaluation after the first\n",
      "    iteration will be within the bounds. If `direc` is not full rank,\n",
      "    then some parameters may not be optimized and the solution is not\n",
      "    guaranteed to be within the bounds.\n",
      "    \n",
      "    Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
      "    algorithm [5]_, [8]_ to minimize a function with variables subject\n",
      "    to bounds. This algorithm uses gradient information; it is also\n",
      "    called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
      "    method described above as it wraps a C implementation and allows\n",
      "    each variable to be given upper and lower bounds.\n",
      "    \n",
      "    **Constrained Minimization**\n",
      "    \n",
      "    Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n",
      "    Constrained Optimization BY Linear Approximation (COBYLA) method\n",
      "    [9]_, [10]_, [11]_. The algorithm is based on linear\n",
      "    approximations to the objective function and each constraint. The\n",
      "    method wraps a FORTRAN implementation of the algorithm. The\n",
      "    constraints functions 'fun' may return either a single number\n",
      "    or an array or list of numbers.\n",
      "    \n",
      "    Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
      "    Least SQuares Programming to minimize a function of several\n",
      "    variables with any combination of bounds, equality and inequality\n",
      "    constraints. The method wraps the SLSQP Optimization subroutine\n",
      "    originally implemented by Dieter Kraft [12]_. Note that the\n",
      "    wrapper handles infinite values in bounds by converting them into\n",
      "    large floating values.\n",
      "    \n",
      "    Method :ref:`trust-constr <optimize.minimize-trustconstr>` is a\n",
      "    trust-region algorithm for constrained optimization. It swiches\n",
      "    between two implementations depending on the problem definition.\n",
      "    It is the most versatile constrained minimization algorithm\n",
      "    implemented in SciPy and the most appropriate for large-scale problems.\n",
      "    For equality constrained problems it is an implementation of Byrd-Omojokun\n",
      "    Trust-Region SQP method described in [17]_ and in [5]_, p. 549. When\n",
      "    inequality constraints  are imposed as well, it swiches to the trust-region\n",
      "    interior point  method described in [16]_. This interior point algorithm,\n",
      "    in turn, solves inequality constraints by introducing slack variables\n",
      "    and solving a sequence of equality-constrained barrier problems\n",
      "    for progressively smaller values of the barrier parameter.\n",
      "    The previously described equality constrained SQP method is\n",
      "    used to solve the subproblems with increasing levels of accuracy\n",
      "    as the iterate gets closer to a solution.\n",
      "    \n",
      "    **Finite-Difference Options**\n",
      "    \n",
      "    For Method :ref:`trust-constr <optimize.minimize-trustconstr>`\n",
      "    the gradient and the Hessian may be approximated using\n",
      "    three finite-difference schemes: {'2-point', '3-point', 'cs'}.\n",
      "    The scheme 'cs' is, potentially, the most accurate but it\n",
      "    requires the function to correctly handles complex inputs and to\n",
      "    be differentiable in the complex plane. The scheme '3-point' is more\n",
      "    accurate than '2-point' but requires twice as many operations.\n",
      "    \n",
      "    **Custom minimizers**\n",
      "    \n",
      "    It may be useful to pass a custom minimization method, for example\n",
      "    when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
      "    or a different library.  You can simply pass a callable as the ``method``\n",
      "    parameter.\n",
      "    \n",
      "    The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
      "    where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "    (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
      "    its contents also passed as `method` parameters pair by pair.  Also, if\n",
      "    `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
      "    `fun` returns just the function values and `jac` is converted to a function\n",
      "    returning the Jacobian.  The method shall return an `OptimizeResult`\n",
      "    object.\n",
      "    \n",
      "    The provided `method` callable must be able to accept (and possibly ignore)\n",
      "    arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "    expand in future versions and then these parameters will be passed to\n",
      "    the method.  You can find an example in the scipy.optimize tutorial.\n",
      "    \n",
      "    .. versionadded:: 0.11.0\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
      "        Minimization. The Computer Journal 7: 308-13.\n",
      "    .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
      "        respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
      "        Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
      "        Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
      "        191-208.\n",
      "    .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
      "       a function of several variables without calculating derivatives. The\n",
      "       Computer Journal 7: 155-162.\n",
      "    .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
      "       Numerical Recipes (any edition), Cambridge University Press.\n",
      "    .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
      "       Springer New York.\n",
      "    .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
      "       Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
      "       Scientific and Statistical Computing 16 (5): 1190-1208.\n",
      "    .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
      "       778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
      "       optimization. ACM Transactions on Mathematical Software 23 (4):\n",
      "       550-560.\n",
      "    .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
      "       1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
      "    .. [9] Powell, M J D. A direct search optimization method that models\n",
      "       the objective and constraint functions by linear interpolation.\n",
      "       1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
      "       and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
      "    .. [10] Powell M J D. Direct search algorithms for optimization\n",
      "       calculations. 1998. Acta Numerica 7: 287-336.\n",
      "    .. [11] Powell M J D. A view of algorithms for optimization without\n",
      "       derivatives. 2007.Cambridge University Technical Report DAMTP\n",
      "       2007/NA03\n",
      "    .. [12] Kraft, D. A software package for sequential quadratic\n",
      "       programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
      "       Center -- Institute for Flight Mechanics, Koln, Germany.\n",
      "    .. [13] Conn, A. R., Gould, N. I., and Toint, P. L.\n",
      "       Trust region methods. 2000. Siam. pp. 169-200.\n",
      "    .. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free\n",
      "       implementation of the GLTR method for iterative solution of\n",
      "       the trust region problem\", :arxiv:`1611.04718`\n",
      "    .. [15] N. Gould, S. Lucidi, M. Roma, P. Toint: \"Solving the\n",
      "       Trust-Region Subproblem using the Lanczos Method\",\n",
      "       SIAM J. Optim., 9(2), 504--525, (1999).\n",
      "    .. [16] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\n",
      "        An interior point algorithm for large-scale nonlinear  programming.\n",
      "        SIAM Journal on Optimization 9.4: 877-900.\n",
      "    .. [17] Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998. On the\n",
      "        implementation of an algorithm for large-scale equality constrained\n",
      "        optimization. SIAM Journal on Optimization 8.3: 682-706.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "    function (and its respective derivatives) is implemented in `rosen`\n",
      "    (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
      "    \n",
      "    >>> from scipy.optimize import minimize, rosen, rosen_der\n",
      "    \n",
      "    A simple application of the *Nelder-Mead* method is:\n",
      "    \n",
      "    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "    >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    \n",
      "    Now using the *BFGS* algorithm, using the first derivative and a few\n",
      "    options:\n",
      "    \n",
      "    >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "    ...                options={'gtol': 1e-6, 'disp': True})\n",
      "    Optimization terminated successfully.\n",
      "             Current function value: 0.000000\n",
      "             Iterations: 26\n",
      "             Function evaluations: 31\n",
      "             Gradient evaluations: 31\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    >>> print(res.message)\n",
      "    Optimization terminated successfully.\n",
      "    >>> res.hess_inv\n",
      "    array([[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
      "           [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
      "           [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
      "           [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
      "           [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]])\n",
      "    \n",
      "    \n",
      "    Next, consider a minimization problem with several constraints (namely\n",
      "    Example 16.4 from [5]_). The objective function is:\n",
      "    \n",
      "    >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
      "    \n",
      "    There are three constraints defined as:\n",
      "    \n",
      "    >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
      "    \n",
      "    And variables must be positive, hence the following bounds:\n",
      "    \n",
      "    >>> bnds = ((0, None), (0, None))\n",
      "    \n",
      "    The optimization problem is solved using the SLSQP method as:\n",
      "    \n",
      "    >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
      "    ...                constraints=cons)\n",
      "    \n",
      "    It should converge to the theoretical solution (1.4 ,1.7).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(minimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6eb2f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = minimize(lambda x: ss.norm.cdf(x, loc = 1, scale = 1) * 0.7 + ss.norm.sf(x, loc = 0, scale = 1) * 0.3, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8fb99a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 0.2530043786361955\n",
       " hess_inv: array([[8.7408667]])\n",
       "      jac: array([1.68010592e-06])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 14\n",
       "      nit: 6\n",
       "     njev: 7\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-0.34728293])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "21c18eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "P1, P2 = 0.3, 0.7\n",
    "mix = ss.rv_discrete(name='mix', values=([1, 2], [P1, P2]))\n",
    "p1 = ss.multivariate_normal([0, 0], [[1, 0.5], [0.5, 1]])\n",
    "p2 = ss.multivariate_normal([2.5, 0], [[1, -0.5], [-0.5, 1]])\n",
    "    \n",
    "def make_data(n):\n",
    "    y = mix.rvs(size=n)\n",
    "    X = np.zeros((n, 2))\n",
    "    X[y == 1] = p1.rvs(size=n)[y == 1]\n",
    "    X[y == 2] = p2.rvs(size=n)[y == 2]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c354530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "n = 10000\n",
    "X, y = make_data(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e7c005a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.54623486,  0.9754703 ],\n",
       "       [ 1.64815558,  0.77974093],\n",
       "       [-1.24629726, -1.03959595],\n",
       "       ...,\n",
       "       [ 0.4347837 ,  0.01473326],\n",
       "       [ 2.5865905 ,  0.23849808],\n",
       "       [ 1.22786866,  2.35185771]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ee0c1f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret2 = minimize(lambda x: ss.norm.cdf(x, 2.5, 1) * 0.3 + ss.norm.sf(x, 0, 1) * 0.7, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c01ff1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 0.0935654428535847\n",
       " hess_inv: array([[5.03881299]])\n",
       "      jac: array([-4.88013029e-07])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 12\n",
       "      nit: 5\n",
       "     njev: 6\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([1.58891667])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8076c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = ret2.x[0]\n",
    "a1 = (p1.pdf((x0, 1)) * 0.3, p2.pdf((x0, 1)) * 0.7)\n",
    "a2 = (p1.pdf((x0, -1)) * 0.3, p2.pdf((x0, -1)) * 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0dd9adf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.015168993425267908, 0.06971284803891131),\n",
       " (0.001823390866472412, 0.020689097236106405))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1, a2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
